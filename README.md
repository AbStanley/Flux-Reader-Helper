# Flux

Flux is a premium, privacy-focused reading assistant application designed to bridge the gap between language learning and fluent reading. It leverages local AI to provide instant key-value translations, context-aware insights, and a natural audio reading experience.

## âœ¨ Core Features

### ğŸ§  Smart Reader
The core reading engine allows for deep interaction with text:
- **Interactive Tokenization**: Text is processed into interactive tokens, allowing users to select individual words or phrases.
- **Context-Aware Dictionary**: Hover over any word to see a translation that considers the surrounding sentence context, not just the isolated definition.
- **Streaming Generation**: Generate custom reading material with AI, featuring real-time streaming feedback and a "glass" overlay to prevent interaction during generation.

### ğŸ”Š Audio Experience
A fully integrated text-to-speech system designed for learners:
- **Neural TTS**: Utilizes the browser's best available voices for natural speech.
- **Karaoke Highlighting**: Words are highlighted in real-time as they are spoken, aiding in pronunciation and tracking.
- **Smart Resume**: Playback remembers your position. If paused or stopped, it resumes intelligently from the start of the last spoken sentence or word.
- **Auto-Voice Selection**: Automatically switches the synthesis voice to match the detected language of the text.

### ğŸ›¡ï¸ Privacy & Local AI
Flux is built to run entirely offline (after setup) using **Ollama**:
- **Zero Data Leakage**: All translations and text generation happen on your local machine.
- **Cost Efficient**: No API keys or usage fees.

## ğŸ— Technical Architecture

Flux is built on **Clean Architecture** and **SOLID principles**, ensuring scalability and maintainability.

### System Design
- **Feature-Based Structure**: The codebase is organized by domain (e.g., `features/reader`, `features/controls`) rather than technical layers, making navigation intuitive.
- **Separation of Concerns**: complex logic is decoupled from UI components.
    - **Presentation**: React components (Views) and granular Zustand stores.
    - **Infrastructure**: Concrete implementations of services (e.g., `OllamaService`).
    - **Core**: Domain interfaces and contracts.

### State Management
We utilize **Zustand** for high-performance, granular state management, split into three focused stores to minimize re-renders:
1.  **`useReaderStore`**: Manages the "book" stateâ€”content, pagination, and selection logic.
2.  **`useTranslationStore`**: Handles transient interaction stateâ€”hover lookups, debouncing, and caching.
3.  **`useAudioStore`**: Controls the media engineâ€”playback state, rates, and synchronization.

### Performance Optimizations
- **Memoization**: Critical components like `ReaderToken` are memoized (`React.memo`) to prevent wasted renders during high-frequency events (like audio highlighting).
- **Scoped Props**: Data is passed intelligently to ensure that changing the state of one word does not force the entire page to re-render.

## ğŸ›  Tech Stack

- **Frontend**: React 19, TypeScript, Vite
- **Styling**: Tailwind CSS, Shadcn/ui, Framer Motion
- **State**: Zustand
- **AI/LLM**: Ollama (Local)
- **Containerization**: Docker, Nginx

## ğŸ“‚ Project Structure

```
src/
â”œâ”€â”€ core/                  # Domain Interfaces & Shared Types
â”œâ”€â”€ infrastructure/        # External Services (API, Audio)
â”œâ”€â”€ presentation/          # UI Layer
â”‚   â”œâ”€â”€ components/        # Shared/Generic Components
â”‚   â”œâ”€â”€ features/          # Domain-Specific Modules
â”‚   â”‚   â”œâ”€â”€ controls/      # Input, Settings, Audio Controls
â”‚   â”‚   â””â”€â”€ reader/        # Main Reading Surface & Logic
â”‚   â”œâ”€â”€ ui/                # Design System (shadcn/ui implementation)
â”‚   â”œâ”€â”€ App.tsx            # Application Root
â”‚   â””â”€â”€ main.tsx           # Entry Point
â””â”€â”€ styles/                # Global CSS
```

## ğŸš€ Getting Started

### Prerequisites
- **Node.js**: v18+
- **Docker**: (Optional, for containerized run)
- **Ollama**: Installed and running (`ollama serve`). Ensure you have pulled the models you intend to use (e.g., `ollama pull llama3`).

### Running Locally
To run the application directly on your host machine:

1.  **Install Dependencies**:
    ```bash
    npm install
    ```
2.  **Start Development Server**:
    ```bash
    npm run dev
    ```
3.  **Build for Production**:
    ```bash
    npm run build
    ```

### Running with Docker
For a consistent environment with a pre-configured Nginx reverse proxy (handles CORS for Ollama automatically):

1.  **Start the Stack**:
    ```bash
    npm run docker:up
    ```
    The app will be available at `http://localhost`.

2.  **Stop the Stack**:
    ```bash
    npm run docker:down
    ```
